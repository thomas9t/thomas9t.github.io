---
title: "EEC289Q (Winter 2026): Neurally Inspired Algorithms and Architectures"
collection: teaching
permalink: /teaching/2026-01-01-eec289q/
excerpt: ""
---

# Course Summary

This course provides an overview of topics at the intersection of neural computation and algorithm design and is intended for students with a solid background in linear algebra and probability. Prior exposure to the basic principles of statistical estimation and optimization will be helpful, but not strictly required. The goals for the course are for students to learn about:

* Mathematically rigorous tools for understanding and analyzing algorithms in neural computation.  
* The use of *randomized methods* in neural computation and algorithms.  
* Other paradigms for learning in addition to the currently dominant statistical one.  
* Connections between topics in the analysis of algorithms and neuroscience/neural computation.  
* Applications of some of the above to developing new kinds of computer hardware.


Course grades will be based on a mix of problem sets, in-class presentations, paper-readings, and a final project.

**Note**: this is **not** a class about deep learning. There are many excellent courses at UCD that cover deep learning, but this is not one of them. This course will focus primarily on the *mathematical analysis* of algorithms in neural computation and related topics in computing. 

# Pre-Requisites

EEC161 (or equivalent background in probability), MAT 22A (or equivalent background in linear algebra). Prior exposure to machine learning, statistics, and optimization will be helpful but not strictly required.

# Grading

Grades will be based on the following components:

* Problem sets (3): 30%  
* Final Paper: 30%  
* Final Presentation: 30%  
* Class participation and feedback on presentations: 10%


You are expected to regularly attend class and engage with paper discussions. You are also required to submit feedback forms for presentations by your peers.

# Final Project

The course final project is fairly open ended. Examples may include:

* A thorough literature review on a topic (related to the course) of your choosing  
* A replication study of a paper of your choosing  
* Implementation of an algorithm from the course in hardware  
* Application of one or more concepts from the course to your own research

Depending on the class size, final projects may end up being completed in groups. 

# Tentative Course Outline

A tentative list of topics is as follows. Topics are subject to change depending on the interests and pace of the class. The boundaries between units are not sharp and some may blend into each other.

**Unit 1**: Introduction

* The relevance of neuroscience to algorithm design.  
* The relevance of algorithm design to neuroscience.  
* Why study theory? What do we want theory to tell us?  
* Papers:  
  * D. Marr. [Vision: a computational investigation into the human representation and processing of visual information](https://direct.mit.edu/books/monograph/3299/VisionA-Computational-Investigation-into-the-Human). *MIT Press*, 1982 (republished 2010). 

**Unit 2**: Algorithmic and Statistical Perspectives on Learning

* Generative Modeling:   
  * Perception as inference  
  * Teaser of sparse coding model  
* Algorithmic Approaches Learning:  
  * Statistical Learning:  
    * Risk, empirical risk minimization  
  * Online Learning:  
    * Regret  
* Papers/Blog Posts:   
  * Leo Breiman: [Statistical Modeling: the Two Cultures](https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full)  
  * Peter Norvig: [On Chomsky and the Two Cultures of Statistical Modeling](https://bpb-us-e1.wpmucdn.com/sites.tufts.edu/dist/d/4865/files/2019/04/Norvig.pdf)  
  * Rich Sutton: [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)  
  * Rodney Brooks: [A Better Lesson](https://rodneybrooks.com/a-better-lesson/)

**Unit 3:** Perceptrons and Hebbian Learning

* Perceptrons:  
  * Geometric guarantees for perceptrons  
  * Statistical guarantees for perceptrons  
  * Regret guarantees for perceptrons  
* PCA and Eigenvalue Problems:  
  * Power method and friends  
  * Ojaâ€™s rule and friends  
* Related topics in ML:  
  * Kernel machines:  
    * Kernel functions  
    * Representer Theorem  
  * Kernel Smoothers:  
    * Kernel density estimation  
    * Kernel regression

**Unit 4:** Random Projection and Friends

* Introduction to sketching: Bloom filters, count-sketch, count-min sketch  
* Johnson Lindenstrauss Lemma:  
  * The JLTs: dense JLTs, sparse JLT, fast JLT.  
  * Subspace embeddings, randomized least-squares.  
* Random feature models:  
  * Random Fourier features, polynomial random features \+ tensor sketching.  
* Related Neural Architectures  
  * Vector Symbolic Architectures  
  * Reservoir Computing  
* Papers:  
  * S. Dasgupta, C. Stevens, S. Navlakha. [A neural algorithm for a fundamental computing problem](https://www.science.org/doi/pdf/10.1126/science.aam9868). *Science*, 2017\.  
  * S. Dasgupta, D. Hattori and S. Navlakha. [A neural theory for counting memories](https://cseweb.ucsd.edu/~dasgupta/papers/DHN22.pdf). *Nature Communications*, 2022\.   
  * K. Clarkson, S. Ubaru, and E. Yang. [Capacity analysis of vector symbolic architectures](https://arxiv.org/pdf/2301.10352). *arXiv preprint, 2023\.*  
  * N. Pham, and R. Pagh. [Fast and scalable polynomial kernels via explicit feature maps](https://dl.acm.org/doi/pdf/10.1145/2487575.2487591). *ACM SIGKDD*. 2013  
  * A. Rahimi, and B. Recht. [Random features for large-scale kernel machines](https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf). *Neural Information Processing Systems*, 2007  
  * T. Plate. [Holographic Reduced Representations](https://pages.ucsd.edu/~msereno/170/readings/06-Holographic.pdf). *IEEE Transactions on Neural Networks.* 1995

**Unit 5**: Sparse Recovery and Friends

* Compressed Sensing/LASSO:  
  * Restricted isometry property and recovery guarantees  
  * Relationship with the JL-property  
  * Sublinear compressed sensing and relationship with group-testing (time permitting)  
* Dictionary Learning  
* Papers:  
  * B. Olshausen, and D. Field. [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](https://www.nature.com/articles/381607a0.pdf). *Nature,* 1996\.  
  * D. Donoho. [Compressed Sensing](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1614066). *IEEE Transactions on Information Theory*, 2006  
  * F. Krahmer, and R. Ward. [New and improved Johnson-Lindenstrauss embeddings via the restricted isometry property](https://epubs.siam.org/doi/pdf/10.1137/100810447). *SIAM Journal on Mathematical Analysis*, 2011\.  
  * A. Gilbert, M. Iwen, and M. Strauss. [Group testing and sparse signal recovery](https://ieeexplore.ieee.org/iel5/5061475/5074342/05074574.pdf). *Proceedings of the 42nd Asilomar Conference on Signals, Systems, and Computers (IEEE)*. 2008\.
    

**Unit 6:** (Time permitting) Beyond McCulloch, Pitts, Rosenblatt

* Sigma-Pi Neurons  
  * Relationship to tensor-sketching  
* Spiking neural networks

Additional Suggested Papers for Final Projects (will be updated periodically)

* Y. Shen, S. Dasgupta and S. Navlakha. [Reducing catastrophic forgetting with associative learning: a lesson from fruit flies](https://cseweb.ucsd.edu/~dasgupta/papers/SDN23.pdf). *Neural Computation*, 35(11): 1797-1819, 2023\.  
* S. Chandra, S. Sharma, R. Chaudhuri, I. Fiete, [Episodic and associative memory from spatial scaffolds in the hippocampus](https://www.nature.com/articles/s41586-024-08392-y.pdf). *Nature*, *2025*.  
* C. Kymn et. al. [Binding in Hippocampal-Entorhinal Circuits Enables Compositionality in Cognitive Maps](https://proceedings.neurips.cc/paper_files/paper/2024/hash/4526cfacdbca6b6e184568dac91bf070-Abstract-Conference.html). *Neural Information Processing Systems*, 2025\.  
* C. Rozell, D. Johnson, R. Baraniuk, B. Olshausen. [Sparse coding via thresholding and local competition in neural circuits](https://direct.mit.edu/neco/article-pdf/20/10/2526/817628/neco.2008.03-07-486.pdf). *Neural Computation*. 2008\.  
* Y. Chen, D. Paiton, B. Olshausen. [The sparse manifold transform](https://proceedings.neurips.cc/paper_files/paper/2018/file/8e19a39c36b8e5e3afd2a3b2692aea96-Paper.pdf). *Advances in Neural Information Processing Systems*. 2018\.  
* T. Ahle et. al. [Oblivious sketching of high-degree polynomial kernels](https://epubs.siam.org/doi/pdf/10.1137/1.9781611975994.9). *Proceedings of the Fourteenth Annual ACM-SIAM Symposium of Discrete Algorithms.* 2020*.*  
* J. von Neumann. [Probabilistic logics and the synthesis of reliable organisms from unreliable components](http://web.mit.edu/6.454/www/papers/pierce_1952.pdf). *Automata Studies*. 1956\.  
* A. Rahimi et. al. [High-dimensional computing as a nanoscalable paradigm.](https://ieeexplore.ieee.org/iel7/8919/8017677/07942066.pdf) *IEEE Transactions on Circuits and Systems I: Regular Papers*, 2017\.  
* H. Peng et. al. [Random Feature Attention](https://arxiv.org/pdf/2103.02143). *ICLR*, 2021  
* M. Lewicki. [Efficient coding of natural sounds](https://www.nature.com/articles/nn831). *Nature Neuroscience*, 2002\.  
* T. Kohonen. [The self-organizing map](https://ieeexplore.ieee.org/document/58325). *Proceedings of the IEEE*, 2002  
* K. Fukushima. [Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf). *Biological Cybernetics*, 1980\.  
* E. Arias-Castro, D. Mason, B. Pelletier. [On the estimation of the gradient lines of a density and the consistency of the mean-shift algorithm](https://jmlr.org/papers/volume17/ariascastro16a/ariascastro16a.pdf). *Journal of Machine Learning Research,* 2016\.

